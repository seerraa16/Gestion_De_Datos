{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport pyodbc\\nimport pandas as pd\\nimport numpy as np\\n\\n#  Conexi√≥n a **Azure SQL**\\nAZURE_SERVER = \\'uaxmathfis.database.windows.net\\'\\nAZURE_DATABASE = \\'usecases\\'\\nAZURE_DRIVER = \\'{ODBC Driver 17 for SQL Server}\\'\\n\\nazure_conn_str = f\"DRIVER={AZURE_DRIVER};SERVER={AZURE_SERVER};DATABASE={AZURE_DATABASE};Authentication=ActiveDirectoryInteractive\"\\n\\n#  Conexi√≥n a **SQL Server LOCAL**\\nLOCAL_SERVER = \\'localhost\\'\\nLOCAL_DATABASE = \\'dwh_case1\\'  \\nLOCAL_DRIVER = \\'{ODBC Driver 17 for SQL Server}\\'\\n\\nlocal_conn_str = f\"DRIVER={LOCAL_DRIVER};SERVER={LOCAL_SERVER};DATABASE={LOCAL_DATABASE};Trusted_Connection=yes;TrustServerCertificate=yes\"\\n\\n#  Consulta SQL en Azure SQL\\nSQL_QUERY = \"\"\"\\nSELECT\\n[Id_Producto]\\n    ,producto.[Code_]\\n    ,producto.[CATEGORIA_ID]\\n    ,producto.[Modelo]\\n    ,fuel.[FUEL]\\n    ,categor√≠a_producto.[Grade_ID]\\n    ,categor√≠a_producto.[Equipamiento]\\n    ,costes.[Modelo]\\n    ,costes.[Costetransporte]\\n    ,costes.[GastosMarketing]\\n    ,costes.[Mantenimiento_medio]\\n    ,costes.[Comisi√≥n_Marca]\\n\\nFROM [DATAEX].[006_producto] producto\\nLEFT JOIN\\n  [DATAEX].[015_fuel] fuel ON producto.Fuel_ID = fuel.Fuel_ID\\nLEFT JOIN\\n  [DATAEX].[014_categor√≠a_producto] categor√≠a_producto ON producto.CATEGORIA_ID = categor√≠a_producto.CATEGORIA_ID\\nLEFT JOIN\\n  [DATAEX].[007_costes] costes ON producto.Modelo = costes.Modelo\\n\"\"\"\\n\\n# üîπ Nombre de la tabla en SQL Server Local\\nNEW_TABLE_NAME = \"DATAEX.PRODUCT_DIM\"\\n\\ntry:\\n    #  Conectar a Azure SQL\\n    print(f\"Conectando a Azure SQL...\")\\n    conn_azure = pyodbc.connect(azure_conn_str)\\n\\n    # üîπ Ejecutar la consulta en Azure SQL\\n    print(f\"Ejecutando consulta en Azure SQL...\")\\n    df = pd.read_sql(SQL_QUERY, conn_azure)\\n\\n    if df.empty:\\n        print(f\" La consulta no devolvi√≥ resultados. No se crear√° la tabla en SQL Server Local.\")\\n    else:\\n        print(f\"   - Datos extra√≠dos: {df.shape[0]} filas\")\\n\\n\\n\\n        #  Convertir NaN en columnas num√©ricas a 0\\n        df = df.fillna(0)\\n\\n        #  Convertir valores num√©ricos problem√°ticos\\n        for col in df.select_dtypes(include=[\\'float64\\']).columns:\\n            df[col] = df[col].astype(np.float32)  # Reducir precisi√≥n\\n\\n        for col in df.select_dtypes(include=[\\'int64\\']).columns:\\n            df[col] = df[col].astype(np.int32)  # Evitar valores fuera de rango\\n\\n        #  Conectar a SQL Server Local\\n        print(f\"Conectando a SQL Server Local...\")\\n        conn_local = pyodbc.connect(local_conn_str)\\n\\n        with conn_local.cursor() as cursor:\\n            # üîπ Eliminar la tabla si ya existe\\n            drop_table_sql = f\"DROP TABLE IF EXISTS {NEW_TABLE_NAME}\"\\n            cursor.execute(drop_table_sql)\\n            conn_local.commit()\\n            print(f\"   - Tabla eliminada si exist√≠a.\")\\n\\n            # üîπ Crear la tabla en SQL Server Local con tipos de datos ajustados\\n            create_table_sql = f\"\"\"\\n            CREATE TABLE {NEW_TABLE_NAME} (\\n                {\\', \\'.join([\\n                    f\\'[{col}] FLOAT\\' if df[col].dtype == np.float32 \\n                    else f\\'[{col}] INT\\' if df[col].dtype == np.int32 \\n                    else f\\'[{col}] NVARCHAR(255)\\' for col in df.columns\\n                ])}\\n            );\\n            \"\"\"\\n            cursor.execute(create_table_sql)\\n            conn_local.commit()\\n            print(f\" Tabla {NEW_TABLE_NAME} creada correctamente en SQL Server Local.\")\\n\\n            # Insertar los datos en SQL Server Local\\n            placeholders = \\', \\'.join([\\'?\\' for _ in df.columns])\\n            insert_sql = f\"INSERT INTO {NEW_TABLE_NAME} VALUES ({placeholders})\"\\n\\n            cursor.fast_executemany = True\\n            cursor.executemany(insert_sql, df.values.tolist())\\n            conn_local.commit()\\n\\n            print(f\" {df.shape[0]} filas insertadas en {NEW_TABLE_NAME}.\")\\n\\nexcept Exception as e:\\n    print(f\" Error: {e}\")\\n\\nfinally:\\n    if \\'conn_azure\\' in locals():\\n        conn_azure.close()\\n    if \\'conn_local\\' in locals():\\n        conn_local.close()\\n\\nprint(\"\\n ¬°Proceso completado!\")\\n'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#  Conexi√≥n a **Azure SQL**\n",
    "AZURE_SERVER = 'uaxmathfis.database.windows.net'\n",
    "AZURE_DATABASE = 'usecases'\n",
    "AZURE_DRIVER = '{ODBC Driver 17 for SQL Server}'\n",
    "\n",
    "azure_conn_str = f\"DRIVER={AZURE_DRIVER};SERVER={AZURE_SERVER};DATABASE={AZURE_DATABASE};Authentication=ActiveDirectoryInteractive\"\n",
    "\n",
    "#  Conexi√≥n a **SQL Server LOCAL**\n",
    "LOCAL_SERVER = 'localhost'\n",
    "LOCAL_DATABASE = 'dwh_case1'  \n",
    "LOCAL_DRIVER = '{ODBC Driver 17 for SQL Server}'\n",
    "\n",
    "local_conn_str = f\"DRIVER={LOCAL_DRIVER};SERVER={LOCAL_SERVER};DATABASE={LOCAL_DATABASE};Trusted_Connection=yes;TrustServerCertificate=yes\"\n",
    "\n",
    "#  Consulta SQL en Azure SQL\n",
    "SQL_QUERY = \"\"\"\n",
    "SELECT\n",
    "[Id_Producto]\n",
    "    ,producto.[Code_]\n",
    "    ,producto.[CATEGORIA_ID]\n",
    "    ,producto.[Modelo]\n",
    "    ,fuel.[FUEL]\n",
    "    ,categor√≠a_producto.[Grade_ID]\n",
    "    ,categor√≠a_producto.[Equipamiento]\n",
    "    ,costes.[Modelo]\n",
    "    ,costes.[Costetransporte]\n",
    "    ,costes.[GastosMarketing]\n",
    "    ,costes.[Mantenimiento_medio]\n",
    "    ,costes.[Comisi√≥n_Marca]\n",
    "\n",
    "FROM [DATAEX].[006_producto] producto\n",
    "LEFT JOIN\n",
    "  [DATAEX].[015_fuel] fuel ON producto.Fuel_ID = fuel.Fuel_ID\n",
    "LEFT JOIN\n",
    "  [DATAEX].[014_categor√≠a_producto] categor√≠a_producto ON producto.CATEGORIA_ID = categor√≠a_producto.CATEGORIA_ID\n",
    "LEFT JOIN\n",
    "  [DATAEX].[007_costes] costes ON producto.Modelo = costes.Modelo\n",
    "\"\"\"\n",
    "\n",
    "# üîπ Nombre de la tabla en SQL Server Local\n",
    "NEW_TABLE_NAME = \"DATAEX.PRODUCT_DIM\"\n",
    "\n",
    "try:\n",
    "    #  Conectar a Azure SQL\n",
    "    print(f\"Conectando a Azure SQL...\")\n",
    "    conn_azure = pyodbc.connect(azure_conn_str)\n",
    "    \n",
    "    # üîπ Ejecutar la consulta en Azure SQL\n",
    "    print(f\"Ejecutando consulta en Azure SQL...\")\n",
    "    df = pd.read_sql(SQL_QUERY, conn_azure)\n",
    "\n",
    "    if df.empty:\n",
    "        print(f\" La consulta no devolvi√≥ resultados. No se crear√° la tabla en SQL Server Local.\")\n",
    "    else:\n",
    "        print(f\"   - Datos extra√≠dos: {df.shape[0]} filas\")\n",
    "\n",
    "\n",
    "\n",
    "        #  Convertir NaN en columnas num√©ricas a 0\n",
    "        df = df.fillna(0)\n",
    "\n",
    "        #  Convertir valores num√©ricos problem√°ticos\n",
    "        for col in df.select_dtypes(include=['float64']).columns:\n",
    "            df[col] = df[col].astype(np.float32)  # Reducir precisi√≥n\n",
    "        \n",
    "        for col in df.select_dtypes(include=['int64']).columns:\n",
    "            df[col] = df[col].astype(np.int32)  # Evitar valores fuera de rango\n",
    "        \n",
    "        #  Conectar a SQL Server Local\n",
    "        print(f\"Conectando a SQL Server Local...\")\n",
    "        conn_local = pyodbc.connect(local_conn_str)\n",
    "        \n",
    "        with conn_local.cursor() as cursor:\n",
    "            # üîπ Eliminar la tabla si ya existe\n",
    "            drop_table_sql = f\"DROP TABLE IF EXISTS {NEW_TABLE_NAME}\"\n",
    "            cursor.execute(drop_table_sql)\n",
    "            conn_local.commit()\n",
    "            print(f\"   - Tabla eliminada si exist√≠a.\")\n",
    "\n",
    "            # üîπ Crear la tabla en SQL Server Local con tipos de datos ajustados\n",
    "            create_table_sql = f\"\"\"\n",
    "            CREATE TABLE {NEW_TABLE_NAME} (\n",
    "                {', '.join([\n",
    "                    f'[{col}] FLOAT' if df[col].dtype == np.float32 \n",
    "                    else f'[{col}] INT' if df[col].dtype == np.int32 \n",
    "                    else f'[{col}] NVARCHAR(255)' for col in df.columns\n",
    "                ])}\n",
    "            );\n",
    "            \"\"\"\n",
    "            cursor.execute(create_table_sql)\n",
    "            conn_local.commit()\n",
    "            print(f\" Tabla {NEW_TABLE_NAME} creada correctamente en SQL Server Local.\")\n",
    "\n",
    "            # Insertar los datos en SQL Server Local\n",
    "            placeholders = ', '.join(['?' for _ in df.columns])\n",
    "            insert_sql = f\"INSERT INTO {NEW_TABLE_NAME} VALUES ({placeholders})\"\n",
    "\n",
    "            cursor.fast_executemany = True\n",
    "            cursor.executemany(insert_sql, df.values.tolist())\n",
    "            conn_local.commit()\n",
    "\n",
    "            print(f\" {df.shape[0]} filas insertadas en {NEW_TABLE_NAME}.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    if 'conn_azure' in locals():\n",
    "        conn_azure.close()\n",
    "    if 'conn_local' in locals():\n",
    "        conn_local.close()\n",
    "\n",
    "print(\"\\n ¬°Proceso completado!\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure connection string.\n",
    "AZURE_SERVER = 'uaxmathfis.database.windows.net'\n",
    "AZURE_DATABASE = 'usecases'\n",
    "AZURE_DRIVER = '{ODBC Driver 17 for SQL Server}'\n",
    "\n",
    "azure_conn_str = f\"DRIVER={AZURE_DRIVER};SERVER={AZURE_SERVER};DATABASE={AZURE_DATABASE};Authentication=ActiveDirectoryInteractive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local connection string.\n",
    "LOCAL_SERVER = 'localhost'\n",
    "LOCAL_DATABASE = 'dwh_case1'\n",
    "LOCAL_DRIVER = '{ODBC Driver 17 for SQL Server}'\n",
    "\n",
    "local_conn_str = f\"DRIVER={LOCAL_DRIVER};SERVER={LOCAL_SERVER};DATABASE={LOCAL_DATABASE};Trusted_Connection=yes;TrustServerCertificate=yes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_folder = \"../Database/Dimensional\"\n",
    "queries = {\n",
    "    \"Dim_geo\": \"Dim_Geo.sql\",\n",
    "    \"Dim_product\": \"Dim_Product.sql\",\n",
    "    \"Dim_time\": \"Dim_Time.sql\",\n",
    "    \"Dim_client\": \"Dim_Cli.sql\",\n",
    "    \"Facts_Table\": \"Dim_Fact.sql\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary keys for each table.\n",
    "primary_keys = {\n",
    "    \"Facts_Table\": [\"CODE\"],\n",
    "    \"Dim_client\": [\"Customer_ID\"],\n",
    "    \"Dim_geo\": [\"TIENDA_ID\"],\n",
    "    \"Dim_product\": [\"Id_Producto\"],\n",
    "    \"Dim_time\": [\"Date\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foreign keys for each table.\n",
    "foreign_keys = {\n",
    "    \"Facts_Table\": {\n",
    "        \"Customer_ID\": \"Dim_client(Customer_ID)\",\n",
    "        \"TIENDA_ID\": \"Dim_geo(TIENDA_ID)\",\n",
    "        \"Id_Producto\": \"Dim_product(Id_Producto)\",\n",
    "        \"Sales_Date\": \"Dim_time(Date)\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_sql(table_name, df):\n",
    "    # Definici√≥n de los tipos de datos SQL para cada columna del DataFrame: Por defecto tipo TEXTO.\n",
    "    col_defs = []\n",
    "    for col in df.columns:\n",
    "        if np.issubdtype(df[col].dtype, np.datetime64):\n",
    "            col_defs.append(f'[{col}] DATE')\n",
    "        elif df[col].dtype == np.float32:\n",
    "            col_defs.append(f'[{col}] FLOAT')\n",
    "        elif df[col].dtype == np.int32:\n",
    "            col_defs.append(f'[{col}] INT')\n",
    "        else:\n",
    "            col_defs.append(f'[{col}] NVARCHAR(255)')\n",
    "\n",
    "    # Agregaci√≥n clave primaria si corresponde.\n",
    "    pk = \", PRIMARY KEY (\" + \", \".join(primary_keys[table_name]) + \")\" if table_name in primary_keys else \"\"\n",
    "    # Agregaci√≥n claves for√°neas si corresponde.\n",
    "    fk = \"\"\n",
    "    if table_name in foreign_keys:\n",
    "        for col, ref in foreign_keys[table_name].items():\n",
    "            fk += f\", FOREIGN KEY ({col}) REFERENCES {ref}\"\n",
    "\n",
    "    return f\"CREATE TABLE {table_name} ({', '.join(col_defs)}{pk}{fk});\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_tables_in_order(cursor, conn):\n",
    "    drop_order = [\"Facts_Table\", \"Dim_time\", \"Dim_product\", \"Dim_geo\", \"Dim_client\"]\n",
    "    for table in drop_order:\n",
    "        # Verifica si la tabla existe en el esquema actual.\n",
    "        check_exists_query = f\"\"\"\n",
    "        IF OBJECT_ID('{table}', 'U') IS NOT NULL\n",
    "            DROP TABLE {table};\n",
    "        \"\"\"\n",
    "        try:\n",
    "            cursor.execute(check_exists_query)\n",
    "            conn.commit()\n",
    "        except Exception as e:\n",
    "            print(f\"Error al intentar eliminar la tabla {table}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexiones correctamente establecidas.\n",
      "\n",
      "Procesando: Dim_geo\n",
      "   - Filas obtenidas: 12\n",
      "   - Columnas: ['TIENDA_ID', 'PROVINCIA_ID', 'ZONA_ID', 'TIENDA_DESC', 'PROV_DESC', 'ZONA']\n",
      "   - Tabla Dim_geo creada correctamente.\n",
      "   - 12 filas insertadas.\n",
      "\n",
      "Procesando: Dim_product\n",
      "Columnas duplicadas en Dim_product: ['Modelo']\n",
      "   - Filas obtenidas: 404\n",
      "   - Columnas: ['Id_Producto', 'Code_', 'CATEGORIA_ID', 'Modelo', 'FUEL', 'Grade_ID', 'Equipamiento', 'Costetransporte', 'GastosMarketing', 'Mantenimiento_medio', 'Comisi√≥n_Marca']\n",
      "   - Tabla Dim_product creada correctamente.\n",
      "   - 404 filas insertadas.\n",
      "\n",
      "Procesando: Dim_time\n",
      "   - Filas obtenidas: 3652\n",
      "   - Columnas: ['Date', 'Anno', 'Annomes', 'Dia', 'Diadelasemana', 'Diadelesemana_desc', 'Festivo', 'Findesemana', 'FinMes', 'InicioMes', 'Laboral', 'Mes', 'Mes_desc', 'Week']\n",
      "   - Tabla Dim_time creada correctamente.\n",
      "   - 3652 filas insertadas.\n",
      "\n",
      "Procesando: Dim_client\n",
      "   - Filas obtenidas: 44053\n",
      "   - Columnas: ['Customer_ID', 'Edad', 'Fecha_nacimiento', 'GENERO', 'CP', 'poblacion', 'provincia', 'STATUS_SOCIAL', 'RENTA_MEDIA_ESTIMADA', 'ENCUESTA_ZONA_CLIENTE_VENTA', 'ENCUESTA_CLIENTE_ZONA_TALLER', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'U2', 'Max_Mosaic_G', 'Max_Mosaic2', 'Renta_Media', 'F2', 'Mosaic_number']\n",
      "   - Tabla Dim_client creada correctamente.\n",
      "   - 44053 filas insertadas.\n",
      "\n",
      "Procesando: Facts_Table\n",
      "   - Filas obtenidas: 58049\n",
      "   - Columnas: ['CODE', 'TIENDA_ID', 'Customer_ID', 'Id_Producto', 'Date', 'Sales_Date', 'PVP', 'MANTENIMIENTO_GRATUITO', 'SEGURO_BATERIA_LARGO_PLAZO', 'FIN_GARANTIA', 'COSTE_VENTA_NO_IMPUESTOS', 'IMPUESTOS', 'EN_GARANTIA', 'EXTENSION_GARANTIA', 'Margen', 'Margendistribuidor', 'Costetransporte', 'GastosMarketing', 'Comisi√≥n_marca', 'Lead_compra', 'fue_Lead', 'DIAS_DESDE_ULTIMA_REVISION', 'Margen_eur_bruto', 'Margen_eur']\n",
      "   - Tabla Facts_Table creada correctamente.\n",
      "   - 58049 filas insertadas.\n",
      "\n",
      "ETL completado.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Conexi√≥n a las bases de datos.\n",
    "    conn_azure = pyodbc.connect(azure_conn_str)\n",
    "    conn_local = pyodbc.connect(local_conn_str)\n",
    "    print(\"Conexiones correctamente establecidas.\\n\")\n",
    "\n",
    "    with conn_local.cursor() as cursor:\n",
    "        drop_tables_in_order(cursor, conn_local)\n",
    "    # Procesamiento de cada tabla definida en el diccionario de Queries.\n",
    "    for table_name, file in queries.items():\n",
    "        print(f\"Procesando: {table_name}\")\n",
    "        query_path = os.path.join(query_folder, file)\n",
    "        with open(query_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            sql_query = f.read()\n",
    "\n",
    "        # Ejecuci√≥n de la consulta sobre la base de datos de Azure.\n",
    "        df = pd.read_sql(sql_query, conn_azure)\n",
    "\n",
    "        # Eliminaci√≥n de las columnas duplicadas.\n",
    "        if df.columns.duplicated().any():\n",
    "            print(f\"Columnas duplicadas en {table_name}: {df.columns[df.columns.duplicated()].tolist()}\")\n",
    "            df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "        # Detecci√≥n de las columnas tipo DATE para convertirlas adecuadamente.\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == object or df[col].dtype == \"string\":\n",
    "                sample_values = df[col].astype(str).sample(min(len(df), 30), random_state=42)\n",
    "                # Saltar si parece una columna num√©rica (para no confundir INT con DATE).\n",
    "                if sample_values.str.isdigit().mean() > 0.8:\n",
    "                    continue\n",
    "                try:\n",
    "                    parsed = pd.to_datetime(sample_values, errors='coerce')\n",
    "                    if parsed.notna().sum() > 0.9 * len(sample_values):\n",
    "                        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "                except:\n",
    "                    pass\n",
    "        # Si el DataFrame est√° vac√≠o, se salta.\n",
    "        if df.empty:\n",
    "            print(f\"La tabla {table_name} no devolvi√≥ resultados.\\n\")\n",
    "            continue\n",
    "        print(f\"   - Filas obtenidas: {df.shape[0]}\")\n",
    "        print(f\"   - Columnas: {df.columns.tolist()}\")\n",
    "\n",
    "        # Limpieza de valores nulos y tipos de datos.\n",
    "        for col in df.columns:\n",
    "            df[col] = df[col].replace(r'^\\s*$', np.nan, regex=True) # Reemplazar espacios en blanco por NaN.\n",
    "            if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                # Valor sentinel (ej: -1 o 999999).\n",
    "                sentinel = -1\n",
    "                df[col] = df[col].fillna(sentinel)\n",
    "            elif pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "                df[col] = df[col].fillna(df[col].mode(dropna=True)[0])\n",
    "            else:\n",
    "                df[col] = df[col].fillna(\"N/A\")\n",
    "        for col in df.select_dtypes(include=['float64']).columns:\n",
    "            df[col] = df[col].astype(np.float32)\n",
    "        for col in df.select_dtypes(include=['int64']).columns:\n",
    "            df[col] = df[col].astype(np.int32)\n",
    "\n",
    "        # Creaci√≥n de la tabla en la base de datos local.\n",
    "        with conn_local.cursor() as cursor:\n",
    "            create_sql = create_table_sql(table_name, df)\n",
    "            cursor.execute(create_sql)\n",
    "            conn_local.commit()\n",
    "            print(f\"   - Tabla {table_name} creada correctamente.\")\n",
    "\n",
    "            placeholders = ', '.join(['?' for _ in df.columns])\n",
    "            insert_sql = f\"INSERT INTO {table_name} VALUES ({placeholders})\"\n",
    "            cursor.fast_executemany = True\n",
    "            cursor.executemany(insert_sql, df.values.tolist())\n",
    "            conn_local.commit()\n",
    "            print(f\"   - {df.shape[0]} filas insertadas.\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    if 'conn_azure' in locals():\n",
    "        conn_azure.close()\n",
    "    if 'conn_local' in locals():\n",
    "        conn_local.close()\n",
    "\n",
    "print(\"ETL completado.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
