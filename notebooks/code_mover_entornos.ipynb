{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure connection string.\n",
    "AZURE_SERVER = 'uaxmathfis.database.windows.net'\n",
    "AZURE_DATABASE = 'usecases'\n",
    "AZURE_DRIVER = '{ODBC Driver 17 for SQL Server}'\n",
    "\n",
    "azure_conn_str = f\"DRIVER={AZURE_DRIVER};SERVER={AZURE_SERVER};DATABASE={AZURE_DATABASE};Authentication=ActiveDirectoryInteractive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local connection string.\n",
    "LOCAL_SERVER = 'localhost'\n",
    "LOCAL_DATABASE = 'dwh_case1'\n",
    "LOCAL_DRIVER = '{ODBC Driver 17 for SQL Server}'\n",
    "\n",
    "local_conn_str = f\"DRIVER={LOCAL_DRIVER};SERVER={LOCAL_SERVER};DATABASE={LOCAL_DATABASE};Trusted_Connection=yes;TrustServerCertificate=yes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_folder = \"../Database/Dimensional\"\n",
    "queries = {\n",
    "    \"Dim_geo\": \"Dim_Geo.sql\",\n",
    "    \"Dim_product\": \"Dim_Product.sql\",\n",
    "    \"Dim_time\": \"Dim_Time.sql\",\n",
    "    \"Dim_client\": \"Dim_Cli.sql\",\n",
    "    \"Facts_Table\": \"Dim_Fact.sql\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary keys for each table.\n",
    "primary_keys = {\n",
    "    \"Facts_Table\": [\"CODE\"],\n",
    "    \"Dim_client\": [\"Customer_ID\"],\n",
    "    \"Dim_geo\": [\"TIENDA_ID\"],\n",
    "    \"Dim_product\": [\"Id_Producto\"],\n",
    "    \"Dim_time\": [\"Date\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foreign keys for each table.\n",
    "foreign_keys = {\n",
    "    \"Facts_Table\": {\n",
    "        \"Customer_ID\": \"Dim_client(Customer_ID)\",\n",
    "        \"TIENDA_ID\": \"Dim_geo(TIENDA_ID)\",\n",
    "        \"Id_Producto\": \"Dim_product(Id_Producto)\",\n",
    "        \"Sales_Date\": \"Dim_time(Date)\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_create_table_sql(table_name, df):\n",
    "    # Determinar los tipos de datos SQL según el tipo de dato de cada columna del DataFrame.\n",
    "    column_definitions = []\n",
    "    for column in df.columns:\n",
    "        if np.issubdtype(df[column].dtype, np.datetime64):\n",
    "            sql_type = \"DATE\"\n",
    "        elif df[column].dtype == np.float32:\n",
    "            sql_type = \"FLOAT\"\n",
    "        elif df[column].dtype == np.int32:\n",
    "            sql_type = \"INT\"\n",
    "        else:\n",
    "            sql_type = \"NVARCHAR(255)\"\n",
    "        column_definitions.append(f\"[{column}] {sql_type}\")\n",
    "\n",
    "    # Definir clave primaria si existe\n",
    "    primary_key = f\", PRIMARY KEY ({', '.join(primary_keys[table_name])})\" if table_name in primary_keys else \"\"\n",
    "\n",
    "    # Definir claves foráneas si existen\n",
    "    foreign_key_constraints = \"\"\n",
    "    if table_name in foreign_keys:\n",
    "        foreign_key_constraints = \"\".join(f\", FOREIGN KEY ({col}) REFERENCES {ref}\" for col, ref in foreign_keys[table_name].items())\n",
    "\n",
    "    # Construir la sentencia SQL final\n",
    "    return f\"CREATE TABLE {table_name} ({', '.join(column_definitions)}{primary_key}{foreign_key_constraints});\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_existing_tables(cursor, conn):\n",
    "    tables_to_drop = [\"Facts_Table\", \"Dim_time\", \"Dim_product\", \"Dim_geo\", \"Dim_client\"]\n",
    "    \n",
    "    for tbl in tables_to_drop:\n",
    "        drop_query = f\"\"\"\n",
    "        IF EXISTS (SELECT 1 FROM sys.objects WHERE object_id = OBJECT_ID('{tbl}') AND type = 'U')\n",
    "            DROP TABLE {tbl};\n",
    "        \"\"\"\n",
    "        try:\n",
    "            cursor.execute(drop_query)\n",
    "            conn.commit()\n",
    "        except Exception as err:\n",
    "            print(f\"Error al eliminar la tabla {tbl}: {err}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexiones establecidas exitosamente.\n",
      "\n",
      "Procesando: Dim_geo\n",
      "   - Filas obtenidas: 12\n",
      "   - Columnas: ['TIENDA_ID', 'PROVINCIA_ID', 'ZONA_ID', 'TIENDA_DESC', 'PROV_DESC', 'ZONA']\n",
      "   - Tabla Dim_geo creada correctamente.\n",
      "   - 12 filas insertadas.\n",
      "\n",
      "Procesando: Dim_product\n",
      "Columnas duplicadas en Dim_product: ['Modelo']\n",
      "   - Filas obtenidas: 404\n",
      "   - Columnas: ['Id_Producto', 'Code_', 'CATEGORIA_ID', 'Modelo', 'FUEL', 'Grade_ID', 'Equipamiento', 'Costetransporte', 'GastosMarketing', 'Mantenimiento_medio', 'Comisión_Marca']\n",
      "   - Tabla Dim_product creada correctamente.\n",
      "   - 404 filas insertadas.\n",
      "\n",
      "Procesando: Dim_time\n",
      "   - Filas obtenidas: 3652\n",
      "   - Columnas: ['Date', 'Anno', 'Annomes', 'Dia', 'Diadelasemana', 'Diadelesemana_desc', 'Festivo', 'Findesemana', 'FinMes', 'InicioMes', 'Laboral', 'Mes', 'Mes_desc', 'Week']\n",
      "   - Tabla Dim_time creada correctamente.\n",
      "   - 3652 filas insertadas.\n",
      "\n",
      "Procesando: Dim_client\n",
      "   - Filas obtenidas: 44053\n",
      "   - Columnas: ['Customer_ID', 'Edad', 'Fecha_nacimiento', 'GENERO', 'CP', 'poblacion', 'provincia', 'lat', 'lon', 'STATUS_SOCIAL', 'RENTA_MEDIA_ESTIMADA', 'ENCUESTA_ZONA_CLIENTE_VENTA', 'ENCUESTA_CLIENTE_ZONA_TALLER', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'U2', 'Max_Mosaic_G', 'Max_Mosaic2', 'Renta_Media', 'F2', 'Mosaic_number']\n",
      "   - Tabla Dim_client creada correctamente.\n",
      "   - 44053 filas insertadas.\n",
      "\n",
      "Procesando: Facts_Table\n",
      "   - Filas obtenidas: 58049\n",
      "   - Columnas: ['CODE', 'TIENDA_ID', 'Customer_ID', 'Id_Producto', 'Date', 'Sales_Date', 'PVP', 'MANTENIMIENTO_GRATUITO', 'SEGURO_BATERIA_LARGO_PLAZO', 'FIN_GARANTIA', 'COSTE_VENTA_NO_IMPUESTOS', 'IMPUESTOS', 'EN_GARANTIA', 'EXTENSION_GARANTIA', 'Margen', 'Margendistribuidor', 'Costetransporte', 'GastosMarketing', 'Comisión_marca', 'Lead_compra', 'fue_Lead', 't_prod_date', 'DIAS_DESDE_ULTIMA_REVISION', 'Car_Age', 'km_ultima_revision', 'Revisiones', 'QUEJA', 'DIAS_EN_TALLER', 'Margen_eur_bruto', 'Margen_eur', 'Churn']\n",
      "   - Tabla Facts_Table creada correctamente.\n",
      "   - 58049 filas insertadas.\n",
      "\n",
      "Proceso ETL finalizado.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Establecer conexión con ambas bases de datos.\n",
    "    azure_conn = pyodbc.connect(azure_conn_str)\n",
    "    local_conn = pyodbc.connect(local_conn_str)\n",
    "    print(\"Conexiones establecidas exitosamente.\\n\")\n",
    "\n",
    "    # Eliminar tablas si existen en la base de datos local.\n",
    "    with local_conn.cursor() as cur:\n",
    "        drop_existing_tables(cur, local_conn)\n",
    "\n",
    "    # Procesar cada tabla definida en el diccionario de consultas.\n",
    "    for tbl_name, filename in queries.items():\n",
    "        print(f\"Procesando: {tbl_name}\")\n",
    "        query_file_path = os.path.join(query_folder, filename)\n",
    "        \n",
    "        with open(query_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            sql_query = file.read()\n",
    "\n",
    "        # Ejecutar la consulta en la base de datos de Azure.\n",
    "        df = pd.read_sql(sql_query, azure_conn)\n",
    "\n",
    "        # Manejo de columnas duplicadas.\n",
    "        if df.columns.duplicated().any():\n",
    "            duplicated_cols = df.columns[df.columns.duplicated()].tolist()\n",
    "            print(f\"Columnas duplicadas en {tbl_name}: {duplicated_cols}\")\n",
    "            df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "        # Convertir columnas de tipo DATE si es necesario.\n",
    "        for column in df.columns:\n",
    "            if df[column].dtype in [object, \"string\"]:\n",
    "                sampled_values = df[column].astype(str).sample(min(len(df), 30), random_state=42)\n",
    "                \n",
    "                # Omitir columnas que parecen numéricas para evitar errores de conversión.\n",
    "                if sampled_values.str.isdigit().mean() > 0.8:\n",
    "                    continue\n",
    "                try:\n",
    "                    parsed_dates = pd.to_datetime(sampled_values, errors='coerce')\n",
    "                    if parsed_dates.notna().sum() > 0.9 * len(sampled_values):\n",
    "                        df[column] = pd.to_datetime(df[column], errors='coerce')\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        # Si el DataFrame está vacío, omitir el procesamiento.\n",
    "        if df.empty:\n",
    "            print(f\"La tabla {tbl_name} no contiene datos.\\n\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"   - Filas obtenidas: {df.shape[0]}\")\n",
    "        print(f\"   - Columnas: {df.columns.tolist()}\")\n",
    "\n",
    "        # Limpiar valores nulos y ajustar tipos de datos.\n",
    "        for column in df.columns:\n",
    "            df[column] = df[column].replace(r'^\\s*$', np.nan, regex=True)  # Reemplazar espacios vacíos con NaN.\n",
    "            if pd.api.types.is_numeric_dtype(df[column]):\n",
    "                df[column] = df[column].fillna(-1)  # Usar -1 como valor sentinel.\n",
    "            elif pd.api.types.is_datetime64_any_dtype(df[column]):\n",
    "                df[column] = df[column].fillna(df[column].mode(dropna=True)[0])\n",
    "            else:\n",
    "                df[column] = df[column].fillna(\"N/A\")\n",
    "\n",
    "        for column in df.select_dtypes(include=['float64']).columns:\n",
    "            df[column] = df[column].astype(np.float32)\n",
    "        for column in df.select_dtypes(include=['int64']).columns:\n",
    "            df[column] = df[column].astype(np.int32)\n",
    "\n",
    "        # Crear tabla en la base de datos local.\n",
    "        with local_conn.cursor() as cur:\n",
    "            create_table_query = generate_create_table_sql(tbl_name, df)\n",
    "            cur.execute(create_table_query)\n",
    "            local_conn.commit()\n",
    "            print(f\"   - Tabla {tbl_name} creada correctamente.\")\n",
    "\n",
    "            # Insertar datos en la tabla creada.\n",
    "            placeholders = ', '.join(['?' for _ in df.columns])\n",
    "            insert_query = f\"INSERT INTO {tbl_name} VALUES ({placeholders})\"\n",
    "            cur.fast_executemany = True\n",
    "            cur.executemany(insert_query, df.values.tolist())\n",
    "            local_conn.commit()\n",
    "            print(f\"   - {df.shape[0]} filas insertadas.\\n\")\n",
    "\n",
    "except Exception as err:\n",
    "    print(f\"Se ha producido un error: {err}\")\n",
    "\n",
    "finally:\n",
    "    if 'azure_conn' in locals():\n",
    "        azure_conn.close()\n",
    "    if 'local_conn' in locals():\n",
    "        local_conn.close()\n",
    "\n",
    "print(\"Proceso ETL finalizado.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
